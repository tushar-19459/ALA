{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "HeWgOD1uD1YP",
   "metadata": {
    "id": "HeWgOD1uD1YP"
   },
   "source": [
    "---\n",
    "\n",
    "Load libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "FXrh9fPyMtwx",
   "metadata": {
    "executionInfo": {
     "elapsed": 15629,
     "status": "ok",
     "timestamp": 1752307312293,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "FXrh9fPyMtwx"
   },
   "outputs": [],
   "source": [
    "## Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DIsc4Twv6UhG",
   "metadata": {
    "id": "DIsc4Twv6UhG"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0s0mudRDMxGf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23149,
     "status": "ok",
     "timestamp": 1752307335436,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "0s0mudRDMxGf",
    "outputId": "799772f4-c7a3-4c07-c8f1-6b35f8b9e250"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/OddSem2025MAHE'\n",
    "    DATA_DIR = DIR + '/Data/'\n",
    "else:\n",
    "    DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gXTvRmG7jzDl",
   "metadata": {
    "id": "gXTvRmG7jzDl"
   },
   "source": [
    "---\n",
    "\n",
    "Load diabetes data (binary & multilabel classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "jd_t_s-pjytm",
   "metadata": {
    "id": "jd_t_s-pjytm",
    "outputId": "16a5eeee-e7cc-4a9f-e10c-7b8f7d20410a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes data set\n",
      "---------------------\n",
      "Number of training samples = 614\n",
      "Number of features = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\miniconda3\\envs\\ALA\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8272\\2047066913.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype = torch.float32)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8272\\2047066913.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "## Load diabetes data\n",
    "file = DATA_DIR+'diabetes.csv'\n",
    "df= pd.read_csv(file, header = 0).dropna()\n",
    "\n",
    "## Train and test split of the data\n",
    "X = df.loc[:,df.columns!='Outcome']\n",
    "y = df['Outcome']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X , y ,test_size=0.2,random_state=1)\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "X_train = torch.tensor(sc.fit_transform(X_train),dtype=torch.float64)\n",
    "X_test = torch.tensor(sc.transform(X_train),dtype=torch.float64)\n",
    "\n",
    "# Convert train and test data to torch tensors (note that Y should be a 1-column matrix)\n",
    "X_train = torch.tensor(X_train, dtype = torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype = torch.float32)\n",
    "# Label encode class labels (do the following if using logistic regression for multilabel classification)\n",
    "Y_train = torch.tensor(Y_train.values,dtype=torch.float64).reshape(-1, 1)\n",
    "Y_test = torch.tensor(Y_test.values,dtype=torch.float64).reshape(-1, 1)\n",
    "\n",
    "# One-hot encode class labels (do the following if using softmax for multilabel classification)\n",
    "#Y_train =  nn.functional.one_hot(torch.tensor(Y_train.values, dtype = torch.int64))\n",
    "#Y_test = nn.functional.one_hot(torch.tensor(Y_test.values, dtype = torch.int64))\n",
    "\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "num_features = X_train.shape[1]\n",
    "num_labels = len(np.unique(Y_train))\n",
    "\n",
    "print('Diabetes data set')\n",
    "print('---------------------')\n",
    "print('Number of training samples = %d'%(num_samples))\n",
    "print('Number of features = %d'%(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4He749TiFAev",
   "metadata": {
    "id": "4He749TiFAev"
   },
   "source": [
    "---\n",
    "\n",
    "Load MNIST Data (multilabel classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a36c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "r0ENTh3fFBBr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1752307336399,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "r0ENTh3fFBBr",
    "outputId": "27954b49-efa3-40ae-aa3e-cb4ca2d1a61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1us/step\n",
      "MNIST set\n",
      "---------------------\n",
      "Number of training samples = 60000\n",
      "Number of features = 784\n",
      "Number of output labels = 10\n"
     ]
    }
   ],
   "source": [
    "## Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = torch.tensor(X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = torch.tensor(X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "num_labels = len(np.unique(y_train))\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# One-hot encode class labels\n",
    "Y_train =  nn.functional.one_hot(torch.tensor(y_train, dtype = torch.int64))\n",
    "Y_test = nn.functional.one_hot(torch.tensor(y_test, dtype = torch.int64))\n",
    "\n",
    "# Normalize the samples (images) using the training data\n",
    "xmax = torch.amax(X_train) # 255\n",
    "xmin = torch.amin(X_train) # 0\n",
    "X_train = (X_train-xmin)/(xmax-xmin) # all train features turn into a number between 0 and 1\n",
    "X_test = (X_test-xmin)/(xmax-xmin)\n",
    "\n",
    "print('MNIST set')\n",
    "print('---------------------')\n",
    "print('Number of training samples = %d'%(num_samples))\n",
    "print('Number of features = %d'%(num_features))\n",
    "print('Number of output labels = %d'%(num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1jRnFbJc9p0h",
   "metadata": {
    "id": "1jRnFbJc9p0h"
   },
   "source": [
    "---\n",
    "\n",
    "Load housing data (regression)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0XeVPqUg9qep",
   "metadata": {
    "id": "0XeVPqUg9qep",
    "outputId": "579f180b-b805-49c1-9a4c-b994fc583887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Housing data set\n",
      "---------------------\n",
      "Number of training samples = 159\n",
      "Number of features = 2\n"
     ]
    }
   ],
   "source": [
    "## Load housing data\n",
    "file = DATA_DIR+'houseprices_cleaned.csv'\n",
    "df= pd.read_csv(file, header = 0).dropna()\n",
    "\n",
    "## Train and test split of the data\n",
    "X = df[['area', 'rent']]\n",
    "y = df['price_per_sqft']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Convert train and test data to numpy arrays (note that Y should be a 1-column matrix)\n",
    "X_train = torch.tensor(X_train, dtype = torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype = torch.float32)\n",
    "Y_train = torch.tensor(Y_train.values, dtype = torch.float32).reshape(-1, 1)\n",
    "Y_test = torch.tensor(Y_test.values, dtype = torch.float32).reshape(-1, 1)\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "print('Housing data set')\n",
    "print('---------------------')\n",
    "print('Number of training samples = %d'%(num_samples))\n",
    "print('Number of features = %d'%(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oKggGU1vRBif",
   "metadata": {
    "id": "oKggGU1vRBif"
   },
   "source": [
    "---\n",
    "\n",
    "A generic layer class with forward and backward methods\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tZHmwpk4Q404",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1752307360887,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "tZHmwpk4Q404"
   },
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Layer, self).__init__()\n",
    "    self.input = None\n",
    "    self.output = None\n",
    "\n",
    "  def forward(self, input):\n",
    "    raise NotImplementedError(\"Forward propagation not implemented\")\n",
    "\n",
    "  def backward(self, output_gradent,learning_rate):\n",
    "    raise NotImplementedError(\"Backward propagation not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hEbjw0C-lHvD",
   "metadata": {
    "id": "hEbjw0C-lHvD"
   },
   "source": [
    "---\n",
    "\n",
    "Binary crossentropy (BCE) loss and its gradient for the batch samples (for binary classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6vScTZbumL8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752307362193,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "f6vScTZbumL8"
   },
   "outputs": [],
   "source": [
    "## Define the loss function and its gradient\n",
    "class BinaryCrossEntropyLoss:\n",
    "  def forward(self, Y, Yhat):\n",
    "    epsilon = torch.tensor(1.0e-08)\n",
    "    return torch.mean(-Y*torch.log(Yhat+epsilon)-(1-Y)*torch.log(1-Yhat+epsilon))#epsilon so that to overcome div by 0 error \n",
    "\n",
    "  def backward(self, Y, Yhat):\n",
    "    epsilon = torch.tensor(1.0e-08)\n",
    "    return (y/(Yhat+epsilon))+((1-y)/(1-(Yhat+epsilon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "McYspVhkFImT",
   "metadata": {
    "id": "McYspVhkFImT"
   },
   "source": [
    "---\n",
    "\n",
    "Categorical crossentropy (CCE) loss and its gradient for the batch samples (for multilabel classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39X45Z-dvG1n",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1752307350748,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "39X45Z-dvG1n"
   },
   "outputs": [],
   "source": [
    "## Define the loss function and its gradient\n",
    "class CategoricalCrossEntropyLoss:\n",
    "  def forward(self, Y, Yhat):\n",
    "    return ?\n",
    "\n",
    "  def backward(self, Y, Yhat):\n",
    "    return ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DPifiTZQ-d0T",
   "metadata": {
    "id": "DPifiTZQ-d0T"
   },
   "source": [
    "---\n",
    "\n",
    "Mean squared error (MSE) loss and its gradient for the batch samples (for regression)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "h0a5D-my-edM",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1752307365441,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "h0a5D-my-edM"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4062546015.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn ?\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Define the loss function and its gradien\n",
    "class MeanSquaredErrorLoss:\n",
    "  def forward(self, Y, Yhat):\n",
    "    return ?\n",
    "\n",
    "  def backward(self, Y, Yhat):\n",
    "    return ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xpWdzn7VzBly",
   "metadata": {
    "id": "xpWdzn7VzBly"
   },
   "source": [
    "---\n",
    "\n",
    "Sigmoid activation layer class\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "W2uYbtc8zFCx",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1752307369315,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "W2uYbtc8zFCx"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "  def forward(self, input):\n",
    "    self.input = input\n",
    "    self.output = 1.0/(1.0+torch.exp(self.input))\n",
    "    return self.output\n",
    "\n",
    "  def backward(self, output_gradient, learning_rate = None):\n",
    "    sigmoid_local_gradient = self.output*(1-self.output)\n",
    "    if output_gradient.shape[1] > 1:\n",
    "      return output_gradient[:, :-1] * sigmoid_local_gradient\n",
    "    else:\n",
    "      return output_gradient * sigmoid_local_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee105f8",
   "metadata": {},
   "source": [
    "----\n",
    "Dense Layer Class\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ab4f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (1526710089.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mclass Dense(Layer):\u001b[39m\n                       ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Dense layer class\n",
    "class Dense(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Dense,self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(input_size+1,output_size)*0.01)\n",
    "        with torch.no_grad():\n",
    "            #set bias to the same costant value\n",
    "            self.weights[-1].fill(0.01)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        #bias trick add 1 to the last columen\n",
    "        self.input = torch.hstack(input,torch.ones(input.shape[0],1))\n",
    "        self.output = torch.matmul(self.input,self.weights)\n",
    "\n",
    "\n",
    "    def backwoard(self,output_gradient,learning_weigth=None):\n",
    "        #local gradient of the dense layer\n",
    "        dense_local_gradient = self.input\n",
    "\n",
    "        #backward flowing gradient from the dense layer\n",
    "        return torch.matmul(dense_local_gradient,output_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t_hwSUUiVlWD",
   "metadata": {
    "id": "t_hwSUUiVlWD"
   },
   "source": [
    "---\n",
    "\n",
    "Softmax activation layer class\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bVplzEcMWlP",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1752307370868,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "6bVplzEcMWlP"
   },
   "outputs": [],
   "source": [
    "## Softmax activation layer class\n",
    "class Softmax(Layer):\n",
    "  def forward(self, input):\n",
    "    self.input = input\n",
    "    exp_values = torch.exp(self.input - torch.max(self.input, dim = 1, keepdim = True).values)\n",
    "    self.output = ?\n",
    "    return self.output\n",
    "\n",
    "  def backward(self, output_gradient, learning_rate = None):\n",
    "    I = torch.eye(self.output.shape[1])\n",
    "    softmax_local_gradient = ?\n",
    "    # Calculate gradient flowing back on the input side of the softmax layer\n",
    "    input_gradient = torch.einsum(?, ?, ?)\n",
    "    return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zg-mDi4jGVj0",
   "metadata": {
    "id": "zg-mDi4jGVj0"
   },
   "source": [
    "---\n",
    "\n",
    "Neural network class for:\n",
    "\n",
    " 1. Binary classification (loss function is binary crossentropy and last layer has one node that is sigmoid-activated).\n",
    " 2. Multilabel classification (loss function is categorical crossentropy and last layer is softmax-activated).\n",
    " 3. Regression (loss function is MSE loss and last layer is dense with one node).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ym5qw3VpGYcM",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752307375162,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "Ym5qw3VpGYcM"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "  def __init__(self, num_features, num_labels, loss_fn,\n",
    "               learning_rate, reg_strength = 0.0):\n",
    "    self.num_features = ?\n",
    "    self.num_labels = ?\n",
    "    self.loss_fn = ?\n",
    "    self.learning_rate = ?\n",
    "    self.reg_strength = ?\n",
    "    self.reg_loss = None\n",
    "    # Architecture\n",
    "    self.layers = ?\n",
    "\n",
    "  # Forward propagation\n",
    "  def forward(self, x):\n",
    "    self.reg_loss = 0.0\n",
    "    for layer in self.layers:\n",
    "      ?\n",
    "    return x\n",
    "\n",
    "  # Backward propagation\n",
    "  def backward(self, loss_gradient):\n",
    "    for layer in ?:\n",
    "      loss_gradient = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_RqlmXuu-RxE",
   "metadata": {
    "id": "_RqlmXuu-RxE"
   },
   "source": [
    "---\n",
    "\n",
    "Train a neural network for:\n",
    "\n",
    " 1. Binary classification (loss function is binary crossentropy).\n",
    " 2. Multilabel classification (loss function is categorical crossentropy).\n",
    " 3. Regression (loss function is MSE).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1HINvJi1Ejyv",
   "metadata": {
    "id": "1HINvJi1Ejyv"
   },
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "learning_rate = ?\n",
    "epochs = ?\n",
    "reg_strength = ?\n",
    "batch_size = ?\n",
    "\n",
    "# Choose appropriate loss function\n",
    "loss_fn = ?\n",
    "\n",
    "# Data loader for batch processing\n",
    "train_loader = ?\n",
    "test_loader = ?\n",
    "\n",
    "model = ?\n",
    "\n",
    "# Create empty list to store training and test losses over each epoch\n",
    "train_loss = [None]*epochs\n",
    "test_loss = [None]*epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  loss = 0.0\n",
    "\n",
    "  for x_batch, y_batch in train_loader:\n",
    "    # Forward pass\n",
    "    predictions = ?\n",
    "    loss += \n",
    "\n",
    "    # Backward pass\n",
    "    loss_gradient = ?\n",
    "    model.backward(?)\n",
    "\n",
    "  train_loss[epoch] = loss.detach().numpy() / len(train_loader)\n",
    "\n",
    "  # Test loss calculation\n",
    "  loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for x_batch, y_batch in ?:\n",
    "      predictions = ?\n",
    "      loss += \n",
    "\n",
    "  test_loss[epoch] = ?\n",
    "  \n",
    "  print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss[epoch]:.4f}, Test Loss: {test_loss[epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8GWSFNvg8arf",
   "metadata": {
    "id": "8GWSFNvg8arf"
   },
   "source": [
    "---\n",
    "\n",
    "Plot training and test loss vs. epoch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S72SmRqD8dca",
   "metadata": {
    "id": "S72SmRqD8dca"
   },
   "outputs": [],
   "source": [
    "## Plot train and test loss as a function of epoch:\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "fig.tight_layout(pad = 4.0)\n",
    "ax.plot(train_loss, 'b', label = 'Train')\n",
    "ax.plot(test_loss, 'r', label = 'Test')\n",
    "ax.set_xlabel('Epoch', fontsize = 12)\n",
    "ax.set_ylabel('Loss value', fontsize = 12)\n",
    "ax.legend()\n",
    "ax.set_title('Loss vs. Epoch', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Eb4-geSICOi",
   "metadata": {
    "id": "3Eb4-geSICOi"
   },
   "source": [
    "---\n",
    "\n",
    "Assess model performance on test data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oo7Cw6sQIDLc",
   "metadata": {
    "id": "oo7Cw6sQIDLc"
   },
   "outputs": [],
   "source": [
    "## Assess model performance on test data (multilabel classification)\n",
    "with torch.no_grad():\n",
    "  for x_batch, y_batch in test_loader:\n",
    "    predictions = ?\n",
    "\n",
    "# Predicted labels for binary classification\n",
    "ypred = ?\n",
    "\n",
    "# Predicted labels for multilabel classification\n",
    "#ypred = ?\n",
    "\n",
    "# Predicted values for regression\n",
    "#ypred = ?\n",
    "\n",
    "# True labels for binary classification\n",
    "ytrue = ?\n",
    "\n",
    "# True labels for multilabel classification\n",
    "#ytrue = ?\n",
    "\n",
    "# True labels for regression\n",
    "#ytrue = ?\n",
    "\n",
    "# Classifier performace\n",
    "?\n",
    "# Print confusion matrix\n",
    "?\n",
    "\n",
    "# Regressor performance\n",
    "#?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
