{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7g7bxFCHxGP"
   },
   "source": [
    "$${\\color{yellow}{\\text{Applied Linear Algebra: Bias trick}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_BbyTKXQflD"
   },
   "source": [
    "---\n",
    "\n",
    "Load essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10608,
     "status": "ok",
     "timestamp": 1751721852571,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "20W0d4ruQjE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "torch.set_printoptions(precision = 3, sci_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfYXkqmLiVLM"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google Drive folder if running Google Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22628,
     "status": "ok",
     "timestamp": 1751721875232,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "VYzBBBxqiaGa",
    "outputId": "235a86c2-d7d2-4d42-d5e0-4a8c57bca3d1"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/OddSem2025MAHE/Share/Applied Linear Algebra'\n",
    "    DATA_DIR = DIR+'/Data/'\n",
    "else:\n",
    "    DATA_DIR = '../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_8cPXFFkiUZ"
   },
   "source": [
    "---\n",
    "\n",
    "Setting up the patient data matrix, true output labels, and the initial weights matrix for the softmax classifier:\n",
    "\n",
    "![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1749395720176,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "MJ3U-JCukmIG",
    "outputId": "88ed6921-4c69-4ef5-9921-1986317a1b26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix: \n",
      "[[ 72.  120.   37.3 104.   32.5]\n",
      " [ 85.  130.   37.  110.   14. ]\n",
      " [ 68.  110.   38.5 125.   34. ]\n",
      " [ 90.  140.   38.  130.   26. ]\n",
      " [ 84.  132.   38.3 146.   30. ]\n",
      " [ 78.  128.   37.2 102.   12. ]]\n",
      "Standardized data matrix: \n",
      "[[-0.979883   -0.70186241 -0.72380201 -0.98707429  0.89204786]\n",
      " [ 0.71858087  0.3509312  -1.24493946 -0.60498101 -1.2373567 ]\n",
      " [-1.50248727 -1.75465602  1.36074779  0.35025217  1.06470228]\n",
      " [ 1.3718362   1.40372481  0.49218537  0.66866323  0.14387869]\n",
      " [ 0.5879298   0.56148993  1.01332282  1.68757862  0.60429048]\n",
      " [-0.1959766   0.14037248 -0.8975145  -1.11443871 -1.4675626 ]]\n",
      "tensor([[0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.100,  0.500,  0.300],\n",
      "        [ 0.900,  0.300,  0.500],\n",
      "        [-1.500,  0.400,  0.100],\n",
      "        [ 0.100,  0.100, -1.000],\n",
      "        [-1.200,  0.500, -0.800]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create the data matrix (read from a file typically)\n",
    "X = np.array([[72, 120, 37.3, 104, 32.5],\n",
    "              [85, 130, 37.0, 110, 14],\n",
    "              [68, 110, 38.5, 125, 34],\n",
    "              [90, 140, 38.0, 130, 26],\n",
    "              [84, 132, 38.3, 146, 30],\n",
    "              [78, 128, 37.2, 102, 12]])\n",
    "print(f'Data matrix: \\n{X}')\n",
    "\n",
    "# Standardize the data matrix\n",
    "sc = StandardScaler() \n",
    "X_S = sc.fit_transform(X)\n",
    "print(f'Standardized data matrix: \\n{X_S}')\n",
    "\n",
    "# Convert to a PyTorch tensor with the standardized values for the data\n",
    "X_S = torch.tensor(X_S,dtype=torch.float64)\n",
    "\n",
    "# Get the number of samples and features\n",
    "num_samples, num_features = X_S.shape\n",
    "\n",
    "# Create the output labels vector (also read from a file typically)\n",
    "y = np.array(['non-diabetic',\n",
    "              'diabetic',\n",
    "              'non-diabetic',\n",
    "              'pre-diabetic',\n",
    "              'diabetic',\n",
    "              'pre-diabetic'])\n",
    "\n",
    "# One-hot encoding of output labels using scikit-learn\n",
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "Y = ohe.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "# Get the number of labels\n",
    "num_labels = Y.shape[1]\n",
    "\n",
    "# Create the weights matrix\n",
    "W = torch.tensor([[-0.1, 0.5, 0.3],\n",
    "                  [0.9, 0.3, 0.5],\n",
    "                  [-1.5, 0.4, 0.1],\n",
    "                  [0.1, 0.1, -1.0],\n",
    "                  [-1.2, 0.5, -0.8]], dtype = torch.float64)\n",
    "\n",
    "print(Y)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cQDyu7llDo9"
   },
   "source": [
    "---\n",
    "\n",
    "Bias trick to absorb the bias into the weights matrix with\n",
    "- ${\\color{yellow}6}$ samples,\n",
    "- ${\\color{green}5}$ features,\n",
    "- ${\\color{cyan}3}$ neurons.\n",
    "\n",
    "$$\\begin{align*}\\mathbf{X} &=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "72 & 120 & 37.3 & 104 & 32.5 \\\\\n",
    "85 & 130 & 37.0 & 110 & 14 \\\\\n",
    "68 & 110 & 38.5 & 125 & 34 \\\\\n",
    "90 & 140 & 38.0 & 130 & 26 \\\\\n",
    "84 & 132 & 38.3 & 146 & 30 \\\\\n",
    "78 & 128 & 37.2 & 102 & 12 \\\\\n",
    "\\end{bmatrix}}_{\\text{original }{\\color{yellow}6}\\times{\\color{green}5}\\text{-data matrix}}\\Rightarrow \\mathbf{X}_B =\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "72 & 120 & 37.3 & 104 & 32.5 & \\color{magenta}{1} \\\\\n",
    "85 & 130 & 37.0 & 110 & 14   & \\color{magenta}{1} \\\\\n",
    "68 & 110 & 38.5 & 125 & 34   & \\color{magenta}{1} \\\\\n",
    "90 & 140 & 38.0 & 130 & 26   & \\color{magenta}{1} \\\\\n",
    "84 & 132 & 38.3 & 146 & 30   & \\color{magenta}{1} \\\\\n",
    "78 & 128 & 37.2 & 102 & 12   & \\color{magenta}{1} \\\\\n",
    "\\end{bmatrix}}_{\\text{bias feature-added }{\\color{yellow}6}\\times({\\color{green}5}+{\\color{magenta}1})\\text{-data matrix}}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\\mathbf{W} &=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "-0.1 & 0.5 & 0.3 \\\\\n",
    "0.9 & 0.3 & 0.5 \\\\\n",
    "-1.5 & 0.4 & 0.1 \\\\\n",
    "0.1 & 0.1 & -1.0 \\\\\n",
    "-1.2 & 0.5 & -0.8 \\\\\n",
    "\\end{bmatrix}}_{\\text{original }{\\color{green}5}\\times{\\color{cyan}3}\\text{-weights matrix}}\n",
    "\\Rightarrow \\mathbf{W}_B=\\underbrace{\\begin{bmatrix}\n",
    "-0.1 & 0.5 & 0.3 \\\\\n",
    "0.9 & 0.3 & 0.5 \\\\\n",
    "-1.5 & 0.4 & 0.1 \\\\\n",
    "0.1 & 0.1 & -1.0 \\\\\n",
    "-1.2 & 0.5 & -0.8 \\\\\n",
    "\\color{magenta}{b_0} & \\color{magenta}{b_1} & \\color{magenta}{b_2} \\\\\n",
    "\\end{bmatrix}}_{\\text{bias row-added }({\\color{green}5}+{\\color{magenta}1})\\times{\\color{cyan}3}\\text{-weights matrix}}\\end{align*}$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_SB = torch.cat([X_S,torch.ones(num_samples,1)],dim=1)\n",
    "# X_SB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.100,  0.500,  0.300],\n",
       "        [ 0.900,  0.300,  0.500],\n",
       "        [-1.500,  0.400,  0.100],\n",
       "        [ 0.100,  0.100, -1.000],\n",
       "        [-1.200,  0.500, -0.800],\n",
       "        [ 0.010,  0.010,  0.010]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.cat([W,0.01*torch.ones(1,num_labels)],dim=0)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1749395871603,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "DlviiS0tlH7p",
    "outputId": "1e8f4378-6e07-4e7d-abd9-18ba1378afd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.980, -0.702, -0.724, -0.987,  0.892,  1.000],\n",
      "        [ 0.719,  0.351, -1.245, -0.605, -1.237,  1.000],\n",
      "        [-1.502, -1.755,  1.361,  0.350,  1.065,  1.000],\n",
      "        [ 1.372,  1.404,  0.492,  0.669,  0.144,  1.000],\n",
      "        [ 0.588,  0.561,  1.013,  1.688,  0.604,  1.000],\n",
      "        [-0.196,  0.140, -0.898, -1.114, -1.468,  1.000]], dtype=torch.float64)\n",
      "tensor([[-0.100,  0.500,  0.300],\n",
      "        [ 0.900,  0.300,  0.500],\n",
      "        [-1.500,  0.400,  0.100],\n",
      "        [ 0.100,  0.100, -1.000],\n",
      "        [-1.200,  0.500, -0.800],\n",
      "        [ 0.010,  0.010,  0.010]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## Bias trick to absorb the bias into the weights matrix\n",
    "# Concatenate a column of ones to X_S (bias term)\n",
    "X_B = torch.cat([X_S,torch.ones(num_samples,1)],dim=1)\n",
    "\n",
    "# Create the bias vector `b`\n",
    "W_B = torch.cat([W,0.01*torch.ones(1,num_labels)],dim=0)\n",
    "\n",
    "# Concatenate the weights matrix `W` with the bias vector `b`\n",
    "print(X_B)\n",
    "print(W_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rZkNr8d1gAw"
   },
   "source": [
    "---\n",
    "\n",
    "Forward propagation for the toy patient dataset: $$\\textbf{bias-added input }\\mathbf{X}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{raw scores }\\mathbf{Z}=\\mathbf{X}_B\\textbf{W}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{softmax activated scores }\\mathbf{A}=\\text{softmax}(\\mathbf{Z}).$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.607, -0.633, -0.434],\n",
       "        [ 3.546, -0.703,  1.871],\n",
       "        [-4.703, -0.156, -2.384],\n",
       "        [ 0.292,  1.453,  0.389],\n",
       "        [-1.620,  1.349, -1.603],\n",
       "        [ 3.152, -1.250,  2.220]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(X_B,W_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1749396348809,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "A5g-D7NLlZBl",
    "outputId": "6ea14681-a9bb-4229-856b-696ac4cdc8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.607, -0.633, -0.434],\n",
      "        [ 3.546, -0.703,  1.871],\n",
      "        [-4.703, -0.156, -2.384],\n",
      "        [ 0.292,  1.453,  0.389],\n",
      "        [-1.620,  1.349, -1.603],\n",
      "        [ 3.152, -1.250,  2.220]], dtype=torch.float64)\n",
      "tensor([[0.316, 0.308, 0.376],\n",
      "        [0.832, 0.012, 0.156],\n",
      "        [0.009, 0.894, 0.096],\n",
      "        [0.189, 0.603, 0.208],\n",
      "        [0.047, 0.906, 0.047],\n",
      "        [0.711, 0.009, 0.280]], dtype=torch.float64)\n",
      "tensor([[0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Raw scores matrix\n",
    "Z = torch.matmul(X_B,W_B) # also referred to as the logits values\n",
    "print(Z)\n",
    "\n",
    "# Softmax activated scores\n",
    "A = F.softmax(Z,dim=1)\n",
    "\n",
    "# Predicted probabilities for each sample\n",
    "print(A)\n",
    "\n",
    "# True output label for each sample\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THWfU_OS6SQF"
   },
   "source": [
    "---\n",
    "\n",
    "Loss for each sample can be quantified using the categorical crossentropy (CCE) loss function which is defined as $$\\color{yellow}{-\\log(\\text{predicted probability that a sample belongs its correct class})}$$\n",
    "\n",
    "For example, consider a sample with\n",
    "\n",
    "- true label = [$\\color{yellow}{1}$ 0 0]\n",
    "- predicted probabilities = [$\\color{yellow}{0.05}$, 0.99, 0.05]\n",
    "\n",
    "$\\Rightarrow$ categorical crossentropy loss = $-\\log(\\color{yellow}{0.05}).$\n",
    "\n",
    "Here, we calculate the CCE loss for all the samples and average them out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 2, 0, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(Y,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1749397032237,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "6a24WlkF6TFz",
    "outputId": "9f374836-be76-403d-a6ed-2b2e0f084252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.230, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.230, dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate average CCE loss\n",
    "loss = torch.mean(-torch.log(torch.sum(Y*A,dim=1)))\n",
    "print(loss)\n",
    "\n",
    "# Using the PyTorch in-built function for CCE loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(Z,torch.argmax(Y,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64)\n",
      "tensor([[0.000, 0.308, 0.000],\n",
      "        [0.832, 0.000, 0.000],\n",
      "        [0.000, 0.894, 0.000],\n",
      "        [0.000, 0.000, 0.208],\n",
      "        [0.047, 0.000, 0.000],\n",
      "        [0.000, 0.000, 0.280]], dtype=torch.float64)\n",
      "tensor([0.308, 0.832, 0.894, 0.208, 0.047, 0.280], dtype=torch.float64)\n",
      "tensor([1.177, 0.184, 0.112, 1.570, 3.067, 1.273], dtype=torch.float64)\n",
      "tensor(1.230, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Y)\n",
    "print(Y*A)\n",
    "print(torch.sum(Y*A,dim=1))\n",
    "print(-torch.log(torch.sum(Y*A,dim=1)))\n",
    "print(torch.mean(-torch.log(torch.sum(Y*A,dim=1))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
